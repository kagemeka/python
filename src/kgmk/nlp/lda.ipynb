{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598591215663",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn \n",
    "import gensim \n",
    "import os \n",
    "cd = os.getcwd(); project_root = f'{cd}'\n",
    "\n",
    "import sys \n",
    "sys.path.append(f'{project_root}/lib')\n",
    "\n",
    "from text_processor import TextProcessor as TP \n",
    "tp = TP(pos=['名詞'])\n",
    "import pandas as pd \n",
    "df = pd.read_csv(f'{project_root}/data/doboku_2019_texts.csv')\n",
    "print(df)\n",
    "ndf = df.dropna()\n",
    "texts = ndf['texts'].map(tp.norm_wakati)\n",
    "\n",
    "# print(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "tokenizer = RegexpTokenizer(r'\\w\\w+')\n",
    "import many_stop_words\n",
    "stop_en = many_stop_words.get_stop_words('en')\n",
    "stop_ja = many_stop_words.get_stop_words('ja')\n",
    "stop_add = set(open(f'{project_root}/data/stopwords.txt').read().split())\n",
    "stop_words = stop_en | stop_ja | stop_add\n",
    "def remove_stop_words(text, stop_words=stop_words):\n",
    "    return [term for term in text if not term in stop_words]\n",
    "\n",
    "\n",
    "texts = texts.map(tokenizer.tokenize).map(remove_stop_words)\n",
    "print(texts)\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "model = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "各トピックの要素\n",
    "'''\n",
    "for topic in model.print_topics(num_topics=3, num_words=10):\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "文章がどのトピックに属するか\n",
    "'''\n",
    "print(texts.values[0])\n",
    "print(model.get_document_topics(corpus[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import csv \n",
    "\n",
    "tmp = []\n",
    "for r in model.get_document_topics(corpus):\n",
    "    tmp2 = []\n",
    "    j = 0\n",
    "    for i, v in r:\n",
    "        while j!=i:\n",
    "            tmp2.append(0)\n",
    "            j += 1\n",
    "        tmp2.append(v)\n",
    "        j += 1\n",
    "    while len(tmp2) < 3:\n",
    "        tmp2.append(0)\n",
    "    tmp.append(tmp2)\n",
    "\n",
    "\n",
    "res = np.array(tmp).T \n",
    "\n",
    "for i in range(len(res)):\n",
    "    ndf[f'score_{i}'] = res[i]\n",
    "print(ndf)\n",
    "print(df)\n",
    "df = df.merge(ndf, how='left', on=['texts', 'file_name'])\n",
    "print(df)\n",
    "df.to_csv(f'{project_root}/data/doboku_2019_texts_lda.csv', index=False, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        "
   ]
  }
 ]
}